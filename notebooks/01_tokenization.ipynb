{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Python 토큰화 (Tokenization)\n",
    "\n",
    "\n",
    "이 노트북에서는 Python 코드를 토큰으로 분해하는 과정인 **토큰화(Tokenization)**에 대해 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 소개: 토큰화란 무엇인가?\n",
    "\n",
    "\n",
    "**토큰화(Tokenization)**는 소스 코드를 의미 있는 최소 단위인 **토큰(Token)**으로 분해하는 과정입니다.\n",
    "\n",
    "Python 인터프리터가 코드를 실행하기 전에 먼저 수행하는 작업으로:\n",
    "- 소스 코드를 읽고\n",
    "- 의미 있는 단위(토큰)로 분리하며\n",
    "- 각 토큰의 타입을 식별합니다\n",
    "\n",
    "### 토큰의 예시\n",
    "\n",
    "코드 `x = 1 + 2`는 다음 토큰들로 분핼됩니다:\n",
    "\n",
    "| 토큰 | 타입 | 설명 |\n",
    "|------|------|------|\n",
    "| `x` | NAME | 변수 이름 |\n",
    "| `=` | OP | 대입 연산자 |\n",
    "| `1` | NUMBER | 숫자 리터럴 |\n",
    "| `+` | OP | 덧셈 연산자 |\n",
    "| `2` | NUMBER | 숫자 리터럴 |\n",
    "\n",
    "Python의 `tokenize` 모듈을 사용하면 직접 토큰화 과정을 관찰할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tokenize 모듈 기본 사용법\n",
    "\n",
    "\n",
    "Python 표준 라이브러리의 `tokenize` 모듈을 사용하여 코드를 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주요 토큰 타입:\n",
      "  NAME: 1 - 변수명, 함수명 등\n",
      "  NUMBER: 2 - 숫자 리터럴\n",
      "  STRING: 3 - 문자열 리터럴\n",
      "  OP: 55 - 연산자\n",
      "  NEWLINE: 4 - 줄바꿈\n",
      "  INDENT: 5 - 들여쓰기 시작\n",
      "  DEDENT: 6 - 들여쓰기 종료\n",
      "  ENDMARKER: 0 - 파일 끝\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "import io\n",
    "\n",
    "# 토큰 타입 이름 확인\n",
    "print(\"주요 토큰 타입:\")\n",
    "print(f\"  NAME: {tokenize.NAME} - 변수명, 함수명 등\")\n",
    "print(f\"  NUMBER: {tokenize.NUMBER} - 숫자 리터럴\")\n",
    "print(f\"  STRING: {tokenize.STRING} - 문자열 리터럴\")\n",
    "print(f\"  OP: {tokenize.OP} - 연산자\")\n",
    "print(f\"  NEWLINE: {tokenize.NEWLINE} - 줄바꿈\")\n",
    "print(f\"  INDENT: {tokenize.INDENT} - 들여쓰기 시작\")\n",
    "print(f\"  DEDENT: {tokenize.DEDENT} - 들여쓰기 종료\")\n",
    "print(f\"  ENDMARKER: {tokenize.ENDMARKER} - 파일 끝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 토큰화 함수\n",
    "\n",
    "\n",
    "`tokenize.generate_tokens(readline)` 함수를 사용합니다:\n",
    "- `readline`: 한 줄씩 읽어오는 함수 (파일 객체의 `readline` 메서드)\n",
    "- 반환: 토큰 네임드튜플 (type, string, start, end, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코드: 'x = 1 + 2'\n",
      "\n",
      "토큰 목록:\n",
      "------------------------------------------------------------\n",
      "  1 NAME         'x'             위치: (1, 0)-(1, 1)\n",
      " 55 OP           '='             위치: (1, 2)-(1, 3)\n",
      "  2 NUMBER       '1'             위치: (1, 4)-(1, 5)\n",
      " 55 OP           '+'             위치: (1, 6)-(1, 7)\n",
      "  2 NUMBER       '2'             위치: (1, 8)-(1, 9)\n",
      "  4 NEWLINE      ''              위치: (1, 9)-(1, 10)\n",
      "  0 ENDMARKER    ''              위치: (2, 0)-(2, 0)\n"
     ]
    }
   ],
   "source": [
    "# 간단한 코드 토큰화 예시\n",
    "code = \"x = 1 + 2\"\n",
    "print(f\"코드: {code!r}\")\n",
    "print(\"\\n토큰 목록:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tokens = tokenize.generate_tokens(io.StringIO(code).readline)\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    print(f\"{tok.type:3} {tok_name:12} {tok.string!r:15} 위치: {tok.start}-{tok.end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 실습 1: 간단한 표현식 토큰화\n",
    "\n",
    "\n",
    "코드 `x = 1 + 2`의 토큰을 상세히 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 실습 1: 간단한 표현식 토큰화 ===\n",
      "\n",
      "원본 코드: 'x = 1 + 2'\n",
      "\n",
      "======================================================================\n",
      "타입              값               시작 위치        끝 위치      \n",
      "======================================================================\n",
      "NAME            'x'             (1, 0)       (1, 1)    \n",
      "OP              '='             (1, 2)       (1, 3)    \n",
      "NUMBER          '1'             (1, 4)       (1, 5)    \n",
      "OP              '+'             (1, 6)       (1, 7)    \n",
      "NUMBER          '2'             (1, 8)       (1, 9)    \n",
      "NEWLINE         ''              (1, 9)       (1, 10)   \n",
      "ENDMARKER       ''              (2, 0)       (2, 0)    \n",
      "\n",
      "분석:\n",
      "- NAME (x): 변수 이름 식별자\n",
      "- OP (=): 대입 연산자\n",
      "- NUMBER (1): 정수 리터럴\n",
      "- OP (+): 덧셈 연산자\n",
      "- NUMBER (2): 정수 리터럴\n",
      "- NEWLINE: 줄의 끝\n",
      "- ENDMARKER: 입력의 끝\n"
     ]
    }
   ],
   "source": [
    "code1 = \"x = 1 + 2\"\n",
    "\n",
    "print(\"=== 실습 1: 간단한 표현식 토큰화 ===\")\n",
    "print(f\"\\n원본 코드: {code1!r}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"{'타입':<15} {'값':<15} {'시작 위치':<12} {'끝 위치':<10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(code1).readline))\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    print(f\"{tok_name:<15} {tok.string!r:<15} {str(tok.start):<12} {str(tok.end):<10}\")\n",
    "\n",
    "print(\"\\n분석:\")\n",
    "print(\"- NAME (x): 변수 이름 식별자\")\n",
    "print(\"- OP (=): 대입 연산자\")\n",
    "print(\"- NUMBER (1): 정수 리터럴\")\n",
    "print(\"- OP (+): 덧셈 연산자\")\n",
    "print(\"- NUMBER (2): 정수 리터럴\")\n",
    "print(\"- NEWLINE: 줄의 끝\")\n",
    "print(\"- ENDMARKER: 입력의 끝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 실습 2: 함수 정의 토큰화\n",
    "\n",
    "\n",
    "함수 정의에서 특별한 토큰인 `INDENT`와 `DEDENT`를 관찰합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 실습 2: 함수 정의 토큰화 ===\n",
      "\n",
      "원본 코드:\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "    return True\n",
      "\n",
      "======================================================================\n",
      "타입              값                    줄     열    \n",
      "======================================================================\n",
      "NAME            'def'                1     0    \n",
      "NAME            'greet'              1     4    \n",
      "OP              '('                  1     9    \n",
      "NAME            'name'               1     10   \n",
      "OP              ')'                  1     14   \n",
      "OP              ':'                  1     15   \n",
      "NEWLINE         '\\n'                 1     16   \n",
      "INDENT          '    '               2     0    \n",
      "NAME            'print'              2     4    \n",
      "OP              '('                  2     9    \n",
      "FSTRING_START   'f\"'                 2     10   \n",
      "FSTRING_MIDDLE  'Hello, '            2     12   \n",
      "OP              '{'                  2     19   \n",
      "NAME            'name'               2     20   \n",
      "OP              '}'                  2     24   \n",
      "FSTRING_MIDDLE  '!'                  2     25   \n",
      "FSTRING_END     '\"'                  2     26   \n",
      "OP              ')'                  2     27   \n",
      "NEWLINE         '\\n'                 2     28   \n",
      "NAME            'return'             3     4    \n",
      "NAME            'True'               3     11   \n",
      "NEWLINE         '\\n'                 3     15   \n",
      "DEDENT          ''                   4     0    \n",
      "ENDMARKER       ''                   4     0    \n",
      "\n",
      "주목할 점:\n",
      "- INDENT: 함수 본문의 들여쓰기가 시작됨을 표시\n",
      "- DEDENT: 들여쓰기가 종료됨을 표시 (return 문 이후)\n",
      "- f-string의 경우 FSTRING_START, FSTRING_MIDDLE, FSTRING_END로 분리됨\n"
     ]
    }
   ],
   "source": [
    "code2 = '''def greet(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "    return True\n",
    "'''\n",
    "\n",
    "print(\"=== 실습 2: 함수 정의 토큰화 ===\")\n",
    "print(f\"\\n원본 코드:\")\n",
    "print(code2)\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'타입':<15} {'값':<20} {'줄':<5} {'열':<5}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(code2).readline))\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    line, col = tok.start\n",
    "    value = tok.string[:18] + '...' if len(tok.string) > 20 else tok.string\n",
    "    print(f\"{tok_name:<15} {value!r:<20} {line:<5} {col:<5}\")\n",
    "\n",
    "print(\"\\n주목할 점:\")\n",
    "print(\"- INDENT: 함수 본문의 들여쓰기가 시작됨을 표시\")\n",
    "print(\"- DEDENT: 들여쓰기가 종료됨을 표시 (return 문 이후)\")\n",
    "print(\"- f-string의 경우 FSTRING_START, FSTRING_MIDDLE, FSTRING_END로 분리됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INDENT와 DEDENT의 동작 원리\n",
    "\n",
    "\n",
    "Python은 들여쓰기를 통해 코드 블록을 구분합니다. 토큰화 과정에서:\n",
    "\n",
    "1. **INDENT**: 들여쓰기 수준이 증가할 때 발생\n",
    "2. **DEDENT**: 들여쓰기 수준이 감소할 때 발생\n",
    "3. 여러 수준의 DEDENT도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 중첩 함수의 INDENT/DEDENT ===\n",
      "def outer():\n",
      "    x = 1\n",
      "    def inner():\n",
      "        y = 2\n",
      "        return y\n",
      "    return x\n",
      "\n",
      "============================================================\n",
      "NAME: 'def'\n",
      "NAME: 'outer'\n",
      "OP: '('\n",
      "OP: ')'\n",
      "OP: ':'\n",
      "INDENT → 레벨 1\n",
      "  NAME: 'x'\n",
      "  OP: '='\n",
      "  NUMBER: '1'\n",
      "  NAME: 'def'\n",
      "  NAME: 'inner'\n",
      "  OP: '('\n",
      "  OP: ')'\n",
      "  OP: ':'\n",
      "  INDENT → 레벨 2\n",
      "    NAME: 'y'\n",
      "    OP: '='\n",
      "    NUMBER: '2'\n",
      "    NAME: 'return'\n",
      "    NAME: 'y'\n",
      "  DEDENT ← 레벨 2\n",
      "  NAME: 'return'\n",
      "  NAME: 'x'\n",
      "DEDENT ← 레벨 1\n"
     ]
    }
   ],
   "source": [
    "# 중첩된 들여쓰기 예시\n",
    "code3 = '''def outer():\n",
    "    x = 1\n",
    "    def inner():\n",
    "        y = 2\n",
    "        return y\n",
    "    return x\n",
    "'''\n",
    "\n",
    "print(\"=== 중첩 함수의 INDENT/DEDENT ===\")\n",
    "print(code3)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(code3).readline))\n",
    "indent_level = 0\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    if tok.type == tokenize.INDENT:\n",
    "        indent_level += 1\n",
    "        print(f\"{'  ' * (indent_level-1)}INDENT → 레벨 {indent_level}\")\n",
    "    elif tok.type == tokenize.DEDENT:\n",
    "        print(f\"{'  ' * (indent_level-1)}DEDENT ← 레벨 {indent_level}\")\n",
    "        indent_level -= 1\n",
    "    elif tok.type in (tokenize.NAME, tokenize.NUMBER, tokenize.OP):\n",
    "        print(f\"{'  ' * indent_level}{tok_name}: {tok.string!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실습 3: 문자열과 f-string 토큰화\n",
    "\n",
    "\n",
    "일반 문자열과 f-string의 토큰화 결과를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반 문자열\n",
    "code4 = 'message = \"Hello, World!\"'\n",
    "\n",
    "print(\"=== 일반 문자열 토큰화 ===\")\n",
    "print(f\"코드: {code4!r}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(code4).readline))\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    print(f\"{tok_name:<15} {tok.string!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f-string\n",
    "code5 = 'result = f\"1 + 2 = {1 + 2}\"'\n",
    "\n",
    "print(\"\\n=== f-string 토큰화 ===\")\n",
    "print(f\"코드: {code5!r}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(code5).readline))\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    print(f\"{tok_name:<20} {tok.string!r}\")\n",
    "\n",
    "print(\"\\nf-string 특수 토큰:\")\n",
    "print(\"- FSTRING_START: f-string 시작 (예: f\\\")\")\n",
    "print(\"- FSTRING_MIDDLE: 문자열 내용\")\n",
    "print(\"- FSTRING_END: f-string 종료 (예: \\\")\")\n",
    "print(\"- LBRACE/RBRACE: 중괄호 { }\")\n",
    "print(\"- 중괄호 안의 표현식은 별도의 토큰으로 분리됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡한 f-string\n",
    "code6 = '''name = \"Alice\"\n",
    "age = 30\n",
    "info = f\"Name: {name.upper()}, Age: {age * 2}\"\n",
    "'''\n",
    "\n",
    "print(\"=== 복잡한 f-string 토큰화 ===\")\n",
    "print(code6)\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'타입':<20} {'값':<25}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(code6).readline))\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    value = tok.string[:22] + '...' if len(tok.string) > 25 else tok.string\n",
    "    print(f\"{tok_name:<20} {value!r:<25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 실습 4: 토큰 테이블 시각화\n",
    "\n",
    "\n",
    "pandas와 rich 라이브러리를 사용하여 토큰 정보를 보기 좋게 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas가 설치되어 있지 않습니다. 기본 출력을 사용합니다.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# pandas가 설치되어 있는지 확인\n",
    "try:\n",
    "    import pandas as pd\n",
    "    HAS_PANDAS = True\n",
    "except ImportError:\n",
    "    HAS_PANDAS = False\n",
    "    print(\"pandas가 설치되어 있지 않습니다. 기본 출력을 사용합니다.\")\n",
    "\n",
    "# rich가 설치되어 있는지 확인\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    HAS_RICH = True\n",
    "    console = Console()\n",
    "except ImportError:\n",
    "    HAS_RICH = False\n",
    "    print(\"rich가 설치되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토큰 테이블 시각화 ===\n",
      "\n",
      "원본 코드:\n",
      "def calculate(x, y):\n",
      "    result = x + y\n",
      "    return result * 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_to_table(code, title=\"토큰 테이블\"):\n",
    "    \"\"\"코드를 토큰화하여 테이블 형태로 반환\"\"\"\n",
    "    tokens = list(tokenize.generate_tokens(io.StringIO(code).readline))\n",
    "    \n",
    "    data = []\n",
    "    for i, tok in enumerate(tokens, 1):\n",
    "        data.append({\n",
    "            '번호': i,\n",
    "            '타입': tokenize.tok_name[tok.type],\n",
    "            '값': tok.string,\n",
    "            '시작(줄,열)': tok.start,\n",
    "            '끝(줄,열)': tok.end,\n",
    "            '원본 줄': tok.line.strip() if tok.line else ''\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 테스트 코드\n",
    "test_code = '''def calculate(x, y):\n",
    "    result = x + y\n",
    "    return result * 2\n",
    "'''\n",
    "\n",
    "print(\"=== 토큰 테이블 시각화 ===\")\n",
    "print(f\"\\n원본 코드:\")\n",
    "print(test_code)\n",
    "\n",
    "token_data = tokenize_to_table(test_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "기본 테이블 출력:\n",
      "번호    타입              값               위치             \n",
      "------------------------------------------------------------\n",
      "1     NAME            'def'           (1, 0)         \n",
      "2     NAME            'calculate'     (1, 4)         \n",
      "3     OP              '('             (1, 13)        \n",
      "4     NAME            'x'             (1, 14)        \n",
      "5     OP              ','             (1, 15)        \n",
      "6     NAME            'y'             (1, 17)        \n",
      "7     OP              ')'             (1, 18)        \n",
      "8     OP              ':'             (1, 19)        \n",
      "9     NEWLINE         '\\n'            (1, 20)        \n",
      "10    INDENT          '    '          (2, 0)         \n",
      "11    NAME            'result'        (2, 4)         \n",
      "12    OP              '='             (2, 11)        \n",
      "13    NAME            'x'             (2, 13)        \n",
      "14    OP              '+'             (2, 15)        \n",
      "15    NAME            'y'             (2, 17)        \n",
      "16    NEWLINE         '\\n'            (2, 18)        \n",
      "17    NAME            'return'        (3, 4)         \n",
      "18    NAME            'result'        (3, 11)        \n",
      "19    OP              '*'             (3, 18)        \n",
      "20    NUMBER          '2'             (3, 20)        \n",
      "21    NEWLINE         '\\n'            (3, 21)        \n",
      "22    DEDENT          ''              (4, 0)         \n",
      "23    ENDMARKER       ''              (4, 0)         \n"
     ]
    }
   ],
   "source": [
    "# pandas를 사용한 테이블 출력\n",
    "if HAS_PANDAS:\n",
    "    df = pd.DataFrame(token_data)\n",
    "    print(\"\\npandas DataFrame:\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"\\n기본 테이블 출력:\")\n",
    "    print(f\"{'번호':<5} {'타입':<15} {'값':<15} {'위치':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for row in token_data:\n",
    "        print(f\"{row['번호']:<5} {row['타입']:<15} {row['값']!r:<15} {str(row['시작(줄,열)']):<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rich를 사용한 예쁜 테이블 출력\n",
    "if HAS_RICH:\n",
    "    table = Table(title=\"토큰 분석 결과\", show_header=True, header_style=\"bold magenta\")\n",
    "    \n",
    "    table.add_column(\"번호\", justify=\"right\", style=\"cyan\", width=4)\n",
    "    table.add_column(\"타입\", style=\"green\", width=15)\n",
    "    table.add_column(\"값\", style=\"yellow\", width=20)\n",
    "    table.add_column(\"시작\", justify=\"center\", width=10)\n",
    "    table.add_column(\"끝\", justify=\"center\", width=10)\n",
    "    \n",
    "    for row in token_data:\n",
    "        table.add_row(\n",
    "            str(row['번호']),\n",
    "            row['타입'],\n",
    "            repr(row['값']),\n",
    "            str(row['시작(줄,열)']),\n",
    "            str(row['끝(줄,열)'])\n",
    "        )\n",
    "    \n",
    "    console.print(table)\n",
    "else:\n",
    "    print(\"rich 라이브러리가 설치되어 있지 않아 예쁜 테이블을 표시할 수 없습니다.\")\n",
    "    print(\"설치: pip install rich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 타입별 색상 구분 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "색상별 토큰 타입:\n",
      "\u001b[94mNAME\u001b[0m: 변수/함수명  \u001b[91mNUMBER\u001b[0m: 숫자  \u001b[92mSTRING\u001b[0m: 문자열  \u001b[93mOP\u001b[0m: 연산자\n",
      "\u001b[96mINDENT/DEDENT\u001b[0m: 들여쓰기  \u001b[90mNEWLINE/END\u001b[0m: 줄끝/파일끝\n",
      "\n",
      "토큰화 결과:\n",
      "------------------------------------------------------------\n",
      "\u001b[94m[NAME]\u001b[0m 'def'\n",
      "\u001b[94m[NAME]\u001b[0m 'hello'\n",
      "\u001b[93m[OP]\u001b[0m '('\n",
      "\u001b[94m[NAME]\u001b[0m 'name'\n",
      "\u001b[93m[OP]\u001b[0m ')'\n",
      "\u001b[93m[OP]\u001b[0m ':'\n",
      "\u001b[90m[NEWLINE]\u001b[0m '\\n'\n",
      "\u001b[90m[COMMENT]\u001b[0m '# 인사말 출력'\n",
      "\u001b[90m[NL]\u001b[0m '\\n'\n",
      "\u001b[96m[INDENT]\u001b[0m '    '\n",
      "\u001b[94m[NAME]\u001b[0m 'msg'\n",
      "\u001b[93m[OP]\u001b[0m '='\n",
      "\u001b[92m[FSTRING_START]\u001b[0m 'f\"'\n",
      "\u001b[92m[FSTRING_MIDDLE]\u001b[0m 'Hi, '\n",
      "\u001b[93m[OP]\u001b[0m '{'\n",
      "\u001b[94m[NAME]\u001b[0m 'name'\n",
      "\u001b[93m[OP]\u001b[0m '}'\n",
      "\u001b[92m[FSTRING_MIDDLE]\u001b[0m '!'\n",
      "\u001b[92m[FSTRING_END]\u001b[0m '\"'\n",
      "\u001b[90m[NEWLINE]\u001b[0m '\\n'\n",
      "\u001b[94m[NAME]\u001b[0m 'print'\n",
      "\u001b[93m[OP]\u001b[0m '('\n",
      "\u001b[94m[NAME]\u001b[0m 'msg'\n",
      "\u001b[93m[OP]\u001b[0m ')'\n",
      "\u001b[90m[NEWLINE]\u001b[0m '\\n'\n",
      "\u001b[94m[NAME]\u001b[0m 'return'\n",
      "\u001b[91m[NUMBER]\u001b[0m '42'\n",
      "\u001b[90m[NEWLINE]\u001b[0m '\\n'\n",
      "\u001b[96m[DEDENT]\u001b[0m ''\n",
      "\u001b[90m[ENDMARKER]\u001b[0m ''\n"
     ]
    }
   ],
   "source": [
    "# 토큰 타입별 색상 매핑\n",
    "TOKEN_COLORS = {\n",
    "    'NAME': '\\033[94m',      # 파란색\n",
    "    'NUMBER': '\\033[91m',    # 빨간색\n",
    "    'STRING': '\\033[92m',    # 초록색\n",
    "    'FSTRING_START': '\\033[92m',\n",
    "    'FSTRING_MIDDLE': '\\033[92m',\n",
    "    'FSTRING_END': '\\033[92m',\n",
    "    'OP': '\\033[93m',        # 노란색\n",
    "    'KEYWORD': '\\033[95m',   # 보라색\n",
    "    'INDENT': '\\033[96m',    # 청록색\n",
    "    'DEDENT': '\\033[96m',\n",
    "    'NEWLINE': '\\033[90m',   # 회색\n",
    "    'ENDMARKER': '\\033[90m',\n",
    "    'NL': '\\033[90m',\n",
    "    'COMMENT': '\\033[90m',\n",
    "    'ENCODING': '\\033[90m',\n",
    "}\n",
    "\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "def colorize_tokens(code):\n",
    "    \"\"\"토큰에 색상을 입혀서 출력\"\"\"\n",
    "    tokens = list(tokenize.generate_tokens(io.StringIO(code).readline))\n",
    "    \n",
    "    print(\"색상별 토큰 타입:\")\n",
    "    print(f\"{TOKEN_COLORS['NAME']}NAME{RESET}: 변수/함수명  \", end=\"\")\n",
    "    print(f\"{TOKEN_COLORS['NUMBER']}NUMBER{RESET}: 숫자  \", end=\"\")\n",
    "    print(f\"{TOKEN_COLORS['STRING']}STRING{RESET}: 문자열  \", end=\"\")\n",
    "    print(f\"{TOKEN_COLORS['OP']}OP{RESET}: 연산자\")\n",
    "    print(f\"{TOKEN_COLORS['INDENT']}INDENT/DEDENT{RESET}: 들여쓰기  \", end=\"\")\n",
    "    print(f\"{TOKEN_COLORS['NEWLINE']}NEWLINE/END{RESET}: 줄끝/파일끝\")\n",
    "    print(\"\\n토큰화 결과:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for tok in tokens:\n",
    "        tok_name = tokenize.tok_name[tok.type]\n",
    "        color = TOKEN_COLORS.get(tok_name, '\\033[97m')\n",
    "        print(f\"{color}[{tok_name}]{RESET} {tok.string!r}\")\n",
    "\n",
    "# 예시 코드\n",
    "sample_code = '''def hello(name):\n",
    "    # 인사말 출력\n",
    "    msg = f\"Hi, {name}!\"\n",
    "    print(msg)\n",
    "    return 42\n",
    "'''\n",
    "\n",
    "colorize_tokens(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 연습 문제\n",
    "\n",
    "\n",
    "직접 코드를 작성하여 토큰화 결과를 확인해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습 1: 클래스 정의 토큰화\n",
    "\n",
    "\n",
    "아래 클래스 정의 코드의 토큰을 분석해보세요.\n",
    "\n",
    "**질문:**\n",
    "1. `class` 키워드는 어떤 토큰 타입인가요?\n",
    "2. `__init__` 메서드의 INDENT/DEDENT는 몇 개씩 있나요?\n",
    "3. `self`는 어떤 토큰 타입으로 분류되나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 연습 1: 클래스 정의 토큰화 ===\n",
      "코드를 분석해보세요!\n",
      "class Person:\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "\n",
      "    def greet(self):\n",
      "        return f\"Hello, I'm {self.name}\"\n",
      "\n",
      "TokenInfo(type=1 (NAME), string='class', start=(1, 0), end=(1, 5), line='class Person:\\n')\n",
      "TokenInfo(type=1 (NAME), string='Person', start=(1, 6), end=(1, 12), line='class Person:\\n')\n",
      "TokenInfo(type=55 (OP), string=':', start=(1, 12), end=(1, 13), line='class Person:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 13), end=(1, 14), line='class Person:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(2, 0), end=(2, 4), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=1 (NAME), string='def', start=(2, 4), end=(2, 7), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=1 (NAME), string='__init__', start=(2, 8), end=(2, 16), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=55 (OP), string='(', start=(2, 16), end=(2, 17), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=1 (NAME), string='self', start=(2, 17), end=(2, 21), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=55 (OP), string=',', start=(2, 21), end=(2, 22), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=1 (NAME), string='name', start=(2, 23), end=(2, 27), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=55 (OP), string=')', start=(2, 27), end=(2, 28), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=55 (OP), string=':', start=(2, 28), end=(2, 29), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 29), end=(2, 30), line='    def __init__(self, name):\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(3, 0), end=(3, 8), line='        self.name = name\\n')\n",
      "TokenInfo(type=1 (NAME), string='self', start=(3, 8), end=(3, 12), line='        self.name = name\\n')\n",
      "TokenInfo(type=55 (OP), string='.', start=(3, 12), end=(3, 13), line='        self.name = name\\n')\n",
      "TokenInfo(type=1 (NAME), string='name', start=(3, 13), end=(3, 17), line='        self.name = name\\n')\n",
      "TokenInfo(type=55 (OP), string='=', start=(3, 18), end=(3, 19), line='        self.name = name\\n')\n",
      "TokenInfo(type=1 (NAME), string='name', start=(3, 20), end=(3, 24), line='        self.name = name\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(3, 24), end=(3, 25), line='        self.name = name\\n')\n",
      "TokenInfo(type=63 (NL), string='\\n', start=(4, 0), end=(4, 1), line='\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(5, 4), end=(5, 4), line='    def greet(self):\\n')\n",
      "TokenInfo(type=1 (NAME), string='def', start=(5, 4), end=(5, 7), line='    def greet(self):\\n')\n",
      "TokenInfo(type=1 (NAME), string='greet', start=(5, 8), end=(5, 13), line='    def greet(self):\\n')\n",
      "TokenInfo(type=55 (OP), string='(', start=(5, 13), end=(5, 14), line='    def greet(self):\\n')\n",
      "TokenInfo(type=1 (NAME), string='self', start=(5, 14), end=(5, 18), line='    def greet(self):\\n')\n",
      "TokenInfo(type=55 (OP), string=')', start=(5, 18), end=(5, 19), line='    def greet(self):\\n')\n",
      "TokenInfo(type=55 (OP), string=':', start=(5, 19), end=(5, 20), line='    def greet(self):\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(5, 20), end=(5, 21), line='    def greet(self):\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(6, 0), end=(6, 8), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=1 (NAME), string='return', start=(6, 8), end=(6, 14), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=59 (FSTRING_START), string='f\"', start=(6, 15), end=(6, 17), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=60 (FSTRING_MIDDLE), string=\"Hello, I'm \", start=(6, 17), end=(6, 28), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=55 (OP), string='{', start=(6, 28), end=(6, 29), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=1 (NAME), string='self', start=(6, 29), end=(6, 33), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=55 (OP), string='.', start=(6, 33), end=(6, 34), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=1 (NAME), string='name', start=(6, 34), end=(6, 38), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=55 (OP), string='}', start=(6, 38), end=(6, 39), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=61 (FSTRING_END), string='\"', start=(6, 39), end=(6, 40), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(6, 40), end=(6, 41), line='        return f\"Hello, I\\'m {self.name}\"\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(7, 0), end=(7, 0), line='')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(7, 0), end=(7, 0), line='')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(7, 0), end=(7, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "# 연습 1 코드\n",
    "ex1_code = '''class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def greet(self):\n",
    "        return f\"Hello, I'm {self.name}\"\n",
    "'''\n",
    "\n",
    "print(\"=== 연습 1: 클래스 정의 토큰화 ===\")\n",
    "print(\"코드를 분석해보세요!\")\n",
    "print(ex1_code)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(ex1_code).readline))\n",
    "for tok in tokens:\n",
    "    print(tok)\n",
    "# 여기에 토큰화 코드를 작성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 연습 1 정답 ===\n",
      "총 토큰 수: 44\n",
      "INDENT 토큰 수: 3\n",
      "DEDENT 토큰 수: 3\n",
      "\n",
      "주요 토큰:\n",
      "  NAME         'class'\n",
      "  NAME         'Person'\n",
      "  NAME         'def'\n",
      "  NAME         '__init__'\n",
      "  NAME         'self'\n",
      "  NAME         'name'\n",
      "  NAME         'self'\n",
      "  NAME         'name'\n",
      "  NAME         'name'\n",
      "  NAME         'def'\n",
      "  NAME         'greet'\n",
      "  NAME         'self'\n",
      "  NAME         'return'\n",
      "  NAME         'self'\n",
      "  NAME         'name'\n"
     ]
    }
   ],
   "source": [
    "# 연습 1 정답 예시\n",
    "print(\"\\n=== 연습 1 정답 ===\")\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(ex1_code).readline))\n",
    "\n",
    "indent_count = sum(1 for t in tokens if t.type == tokenize.INDENT)\n",
    "dedent_count = sum(1 for t in tokens if t.type == tokenize.DEDENT)\n",
    "\n",
    "print(f\"총 토큰 수: {len(tokens)}\")\n",
    "print(f\"INDENT 토큰 수: {indent_count}\")\n",
    "print(f\"DEDENT 토큰 수: {dedent_count}\")\n",
    "print(\"\\n주요 토큰:\")\n",
    "\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    if tok.string in ('class', 'def', 'self', 'return', 'Person', '__init__', 'name', 'greet'):\n",
    "        print(f\"  {tok_name:<12} {tok.string!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습 2: 리스트 컴프리헨션 토큰화\n",
    "\n",
    "\n",
    "리스트 컴프리헨션의 토큰화 결과를 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 연습 2: 리스트 컴프리헨션 토큰화 ===\n",
      "코드: squares = [x**2 for x in range(10) if x % 2 == 0]\n",
      "\n",
      "토큰 목록:\n",
      "--------------------------------------------------\n",
      "  NAME            'squares'\n",
      "  OP              '='\n",
      "  OP              '['\n",
      "  NAME            'x'\n",
      "  OP              '**'\n",
      "  NUMBER          '2'\n",
      "  NAME            'for'\n",
      "  NAME            'x'\n",
      "  NAME            'in'\n",
      "  NAME            'range'\n",
      "  OP              '('\n",
      "  NUMBER          '10'\n",
      "  OP              ')'\n",
      "  NAME            'if'\n",
      "  NAME            'x'\n",
      "  OP              '%'\n",
      "  NUMBER          '2'\n",
      "  OP              '=='\n",
      "  NUMBER          '0'\n",
      "  OP              ']'\n"
     ]
    }
   ],
   "source": [
    "# 연습 2 코드\n",
    "ex2_code = \"squares = [x**2 for x in range(10) if x % 2 == 0]\"\n",
    "\n",
    "print(\"=== 연습 2: 리스트 컴프리헨션 토큰화 ===\")\n",
    "print(f\"코드: {ex2_code}\")\n",
    "print(\"\\n토큰 목록:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tokens = list(tokenize.generate_tokens(io.StringIO(ex2_code).readline))\n",
    "for tok in tokens:\n",
    "    tok_name = tokenize.tok_name[tok.type]\n",
    "    if tok_name not in ('ENCODING', 'NEWLINE', 'ENDMARKER'):\n",
    "        print(f\"  {tok_name:<15} {tok.string!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습 3: 직접 코드 작성해보기\n",
    "\n",
    "\n",
    "아래 셀에 원하는 Python 코드를 작성하고 토큰화 결과를 확인해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 내 코드 토큰화 ===\n",
      "\n",
      "# 여기에 코드를 작성하세요\n",
      "# 예: 조건문, 반복문, 예외 처리 등\n",
      "\n",
      "\n",
      "토큰 목록:\n",
      "--------------------------------------------------\n",
      "NL              '\\n'\n",
      "COMMENT         '# 여기에 코드를 작성하세요'\n",
      "NL              '\\n'\n",
      "COMMENT         '# 예: 조건문, 반복문, 예외 처리 등'\n",
      "NL              '\\n'\n",
      "ENDMARKER       ''\n"
     ]
    }
   ],
   "source": [
    "# 여기에 직접 코드를 작성해보세요\n",
    "my_code = '''\n",
    "# 여기에 코드를 작성하세요\n",
    "# 예: 조건문, 반복문, 예외 처리 등\n",
    "'''\n",
    "\n",
    "print(\"=== 내 코드 토큰화 ===\")\n",
    "print(my_code)\n",
    "\n",
    "# 토큰화 실행\n",
    "if my_code.strip():\n",
    "    tokens = list(tokenize.generate_tokens(io.StringIO(my_code).readline))\n",
    "    print(\"\\n토큰 목록:\")\n",
    "    print(\"-\" * 50)\n",
    "    for tok in tokens:\n",
    "        tok_name = tokenize.tok_name[tok.type]\n",
    "        print(f\"{tok_name:<15} {tok.string!r}\")\n",
    "else:\n",
    "    print(\"코드를 작성해주세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 요약\n",
    "\n",
    "\n",
    "이번 모듈에서 배운 내용:\n",
    "\n",
    "1. **토큰화(Tokenization)**: 소스 코드를 의미 있는 단위(토큰)로 분해하는 과정\n",
    "\n",
    "2. **주요 토큰 타입**:\n",
    "   - `NAME`: 변수명, 함수명 등 식별자\n",
    "   - `NUMBER`: 숫자 리터럴\n",
    "   - `STRING`: 문자열 리터럴\n",
    "   - `OP`: 연산자\n",
    "   - `INDENT`/`DEDENT`: 들여쓰기 시작/종료\n",
    "   - `NEWLINE`: 줄바꿈\n",
    "   - `ENDMARKER`: 파일 끝\n",
    "\n",
    "3. **f-string 토큰화**: 중괄호 안의 표현식이 별도 토큰으로 분리됨\n",
    "\n",
    "4. **tokenize 모듈 사용법**:\n",
    "   ```python\n",
    "   import tokenize\n",
    "   import io\n",
    "   tokens = tokenize.generate_tokens(io.StringIO(code).readline)\n",
    "   ```\n",
    "\n",
    "다음 모듈에서는 토큰을 파싱하여 AST(Abstract Syntax Tree)를 구축하는 과정을 배웁니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
